{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "funny-endorsement",
   "metadata": {},
   "source": [
    "# Label Leakage \n",
    "\n",
    "Label Leakage (Oscar et al.) is one of the weaknesses in SplitNN, and it means that the intermediate gradient which the server sends to the client may be able to allow the client to extract the private ground-truth labels that the server has. We currently support measuring leak_auc that measures how well the l2 norm of the communicated gradient can predict y by the AUC of the ROC curve. Also, we allow you to avoid this leakage with the defense method called *max norm*.\n",
    "\n",
    "reference   \n",
    "https://arxiv.org/abs/2102.08504"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "comparable-height",
   "metadata": {},
   "outputs": [],
   "source": [
    "from attacksplitnn.splitnn import Client, Server, SplitNN\n",
    "from attacksplitnn.defense import max_norm\n",
    "from attacksplitnn.attack import NormAttack\n",
    "from attacksplitnn.utils import DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "governing-parks",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "published-trick",
   "metadata": {},
   "source": [
    "## Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "conceptual-charles",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"batch_size\":128\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "addressed-prerequisite",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brown-hazard",
   "metadata": {},
   "source": [
    "## Load and preprocess data\n",
    "\n",
    "we use the [\"Credit Card Fraud Detection\"](https://www.kaggle.com/mlg-ulb/creditcardfraud) dataset.\n",
    "\n",
    "reference  \n",
    "https://www.tensorflow.org/tutorials/structured_data/imbalanced_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "sharp-dayton",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df = pd.read_csv('https://storage.googleapis.com/download.tensorflow.org/data/creditcard.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "corporate-communist",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df_neg = raw_df[raw_df[\"Class\"] == 0]\n",
    "raw_df_pos = raw_df[raw_df[\"Class\"] == 1]\n",
    "\n",
    "down_df_neg = raw_df_neg#.sample(40000)\n",
    "down_df = pd.concat([down_df_neg, raw_df_pos])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "whole-sport",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples:\n",
      "    Total: 284807\n",
      "    Positive: 492 (0.17% of total)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "neg, pos = np.bincount(down_df['Class'])\n",
    "total = neg + pos\n",
    "print('Examples:\\n    Total: {}\\n    Positive: {} ({:.2f}% of total)\\n'.format(\n",
    "    total, pos, 100 * pos / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "touched-australian",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df = down_df.copy()\n",
    "\n",
    "# You don't want the `Time` column.\n",
    "cleaned_df.pop('Time')\n",
    "\n",
    "# The `Amount` column covers a huge range. Convert to log-space.\n",
    "eps = 0.001 # 0 => 0.1Â¢\n",
    "cleaned_df['Log Ammount'] = np.log(cleaned_df.pop('Amount')+eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aggregate-berlin",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a utility from sklearn to split and shuffle our dataset.\n",
    "train_df, test_df = train_test_split(cleaned_df, test_size=0.2)\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.2)\n",
    "\n",
    "# Form np arrays of labels and features.\n",
    "train_labels = np.array(train_df.pop('Class'))\n",
    "bool_train_labels = train_labels != 0\n",
    "val_labels = np.array(val_df.pop('Class'))\n",
    "test_labels = np.array(test_df.pop('Class'))\n",
    "\n",
    "train_features = np.array(train_df)\n",
    "val_features = np.array(val_df)\n",
    "test_features = np.array(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "killing-netscape",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training labels shape: (182276,)\n",
      "Validation labels shape: (45569,)\n",
      "Test labels shape: (56962,)\n",
      "Training features shape: (182276, 29)\n",
      "Validation features shape: (45569, 29)\n",
      "Test features shape: (56962, 29)\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "train_features = scaler.fit_transform(train_features)\n",
    "\n",
    "val_features = scaler.transform(val_features)\n",
    "test_features = scaler.transform(test_features)\n",
    "\n",
    "train_features = np.clip(train_features, -5, 5)\n",
    "val_features = np.clip(val_features, -5, 5)\n",
    "test_features = np.clip(test_features, -5, 5)\n",
    "\n",
    "\n",
    "print('Training labels shape:', train_labels.shape)\n",
    "print('Validation labels shape:', val_labels.shape)\n",
    "print('Test labels shape:', test_labels.shape)\n",
    "\n",
    "print('Training features shape:', train_features.shape)\n",
    "print('Validation features shape:', val_features.shape)\n",
    "print('Test features shape:', test_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "civilian-permit",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = DataSet(train_features,\n",
    "                        train_labels.astype(np.float64).reshape(-1, 1))\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                           batch_size=config[\"batch_size\"],\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_dataset = DataSet(test_features,\n",
    "                       test_labels.astype(np.float64).reshape(-1, 1))\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                          batch_size=config[\"batch_size\"],\n",
    "                                          shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accepting-anime",
   "metadata": {},
   "source": [
    "## Train SplitNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "expected-racing",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 16\n",
    "\n",
    "class FirstNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FirstNet, self).__init__()        \n",
    "        self.L1 = nn.Linear(train_features.shape[-1],\n",
    "                            hidden_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.L1(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        return x\n",
    "    \n",
    "class SecondNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SecondNet, self).__init__()        \n",
    "        self.L2 = nn.Linear(hidden_dim,\n",
    "                            1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.L2(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x\n",
    "    \n",
    "def torch_auc(label, pred):\n",
    "    return roc_auc_score(label.detach().numpy(),\n",
    "                         pred.detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "banner-potential",
   "metadata": {},
   "source": [
    "### Vanila SplitNN\n",
    "\n",
    "You can see that the leak auc is high, and it indicates that the client can correctly predict the label of each data by analyzing the intermediate gradient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "intensive-equilibrium",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.00057556, metric 0.8556815146688663\n",
      "epoch 2, loss 3.4376e-05, metric 0.9519125572980848\n",
      "epoch 3, loss 2.683e-05, metric 0.9642846761368812\n"
     ]
    }
   ],
   "source": [
    "model_1 = FirstNet()\n",
    "model_1 = model_1.to(device)\n",
    "\n",
    "model_2 = SecondNet()\n",
    "model_2 = model_2.to(device)\n",
    "\n",
    "model_1.double()\n",
    "model_2.double()\n",
    "\n",
    "opt_1 = optim.Adam(model_1.parameters(), lr=1e-3)\n",
    "opt_2 = optim.Adam(model_2.parameters(), lr=1e-3)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "client = Client(model_1, opt_1)\n",
    "server = Server(model_2, opt_2, criterion)\n",
    "\n",
    "sn = SplitNN(client, server, device=device)\n",
    "sn.fit(train_loader, 3, metric=torch_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "varied-fifth",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9996880429021411"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nall = NormAttack(sn)\n",
    "nall.attack(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "composed-grass",
   "metadata": {},
   "source": [
    "### SplitNN with max_norm\n",
    "\n",
    "You can mitigate this problem with max_norm. To use this defense method, you should create a custom Server class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "starting-piece",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Server_with_max_norm(Server):\n",
    "    def __init__(self, server_model,\n",
    "                        server_optimizer,\n",
    "                        criterion):\n",
    "                super().__init__(server_model,\n",
    "                                server_optimizer,\n",
    "                                criterion)\n",
    "\n",
    "    def _fit_server(self, intermidiate_to_server, labels):\n",
    "        outputs = self.server_model(intermidiate_to_server)\n",
    "        loss = self.criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        grad_to_client = intermidiate_to_server.grad.clone()\n",
    "        # use max_norm to avoid label_leakage\n",
    "        grad_to_client = max_norm(grad_to_client)\n",
    "        return outputs, loss, grad_to_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "directed-clear",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.0019253, metric 0.39933885381123874\n",
      "epoch 2, loss 0.00048352, metric 0.3957504813255579\n",
      "epoch 3, loss 0.00026598, metric 0.37047358052584756\n",
      "epoch 4, loss 0.00018096, metric 0.42428235421761884\n",
      "epoch 5, loss 0.00014116, metric 0.5310992900563984\n",
      "epoch 6, loss 0.00012492, metric 0.5898111125748273\n",
      "epoch 7, loss 0.00011209, metric 0.6665164133981102\n",
      "epoch 8, loss 0.00010118, metric 0.7440003788580593\n",
      "epoch 9, loss 9.1988e-05, metric 0.7948218726561581\n",
      "epoch 10, loss 8.5015e-05, metric 0.8192678479651797\n"
     ]
    }
   ],
   "source": [
    "model_1 = FirstNet()\n",
    "model_1 = model_1.to(device)\n",
    "\n",
    "model_2 = SecondNet()\n",
    "model_2 = model_2.to(device)\n",
    "\n",
    "model_1.double()\n",
    "model_2.double()\n",
    "\n",
    "opt_1 = optim.Adam(model_1.parameters(), lr=1e-3)\n",
    "opt_2 = optim.Adam(model_2.parameters(), lr=1e-3)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "client = Client(model_1, opt_1)\n",
    "server = Server_with_max_norm(model_2, opt_2, criterion)\n",
    "\n",
    "sn = SplitNN(client, server, device=device)\n",
    "sn.fit(train_loader, 10, metric=torch_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "preceding-genome",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "leak auc:  0.010744160103578102\n"
     ]
    }
   ],
   "source": [
    "nall = NormAttack(sn)\n",
    "print(\"leak auc: \", nall.attack(train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collaborative-resolution",
   "metadata": {},
   "source": [
    "### SplitNN with dp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "hearing-screening",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kanka\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\opacus\\privacy_engine.py:522: UserWarning: A ``sample_rate`` has been provided.Thus, the provided ``batch_size``and ``sample_size`` will be ignored.\n",
      "  warnings.warn(\n",
      "C:\\Users\\kanka\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\opacus\\privacy_engine.py:194: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n",
      "  warnings.warn(\n",
      "C:\\Users\\kanka\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:795: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.00076693, metric 0.8473859898325028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kanka\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:795: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2, loss 3.3336e-05, metric 0.9525708938585147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kanka\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:795: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3, loss 2.4565e-05, metric 0.972809388894352\n"
     ]
    }
   ],
   "source": [
    "from opacus import PrivacyEngine\n",
    "\n",
    "model_1 = FirstNet()\n",
    "model_1 = model_1.to(device)\n",
    "\n",
    "model_2 = SecondNet()\n",
    "model_2 = model_2.to(device)\n",
    "\n",
    "model_1.double()\n",
    "model_2.double()\n",
    "\n",
    "opt_1 = optim.Adam(model_1.parameters(), lr=1e-3)\n",
    "opt_2 = optim.Adam(model_2.parameters(), lr=1e-3)\n",
    "\n",
    "privacy_engine = PrivacyEngine(\n",
    "    model_2,\n",
    "    sample_rate=0.01,\n",
    "    alphas=[10, 100],\n",
    "    noise_multiplier=1.3,\n",
    "    max_grad_norm=1.0,\n",
    ")\n",
    "privacy_engine.attach(opt_2)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "client = Client(model_1, opt_1)\n",
    "server = Server(model_2, opt_2, criterion)\n",
    "\n",
    "sn = SplitNN(client, server, device=device)\n",
    "sn.fit(train_loader, 3, metric=torch_auc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
